import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score

salary_data = pd.read_csv(r"salary_data.csv")
# Extract feature and target
X = salary_data["YearsExperience"].values
y = salary_data["Salary"].values

# Normalize feature
X = (X - np.mean(X)) / np.std(X)

# Add bias term
X = np.c_[np.ones(X.shape[0]), X]

# Initialize parameters
theta = np.zeros(X.shape[1])
learning_rate = 0.01
epochs = 1000
m = len(y)

# Gradient Descent
loss_history = []
for epoch in range(epochs):
    predictions = X.dot(theta)
    errors = predictions - y
    gradient = (1 / m) * X.T.dot(errors)
    theta -= learning_rate * gradient
    loss = (1 / (2 * m)) * np.sum(errors ** 2)
    loss_history.append(loss)

    # Plot hypothesis after every 100 epochs
    if epoch % 100 == 0:
        plt.scatter(X[:, 1], y, color='blue', label='Actual Data')
        plt.plot(X[:, 1], predictions, color='red', label=f'Epoch {epoch}')
        plt.xlabel("Years of Experience (Normalized)")
        plt.ylabel("Salary")
        plt.legend()
        plt.show()

# Predictions
y_pred = X.dot(theta)

# Evaluate model
r2 = r2_score(y, y_pred)
print(f"Final RÂ² Score: {r2:.4f}")

# Plot loss curve
plt.plot(range(epochs), loss_history, color='purple')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.show()
